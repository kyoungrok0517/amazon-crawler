{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AmazonCorpus(corpora.TextCorpus):\n",
    "    def __init__(self, question_file, tokenizer):\n",
    "        # The stack-overflow questons are stored in a file, one question per line\n",
    "        self.question_file = question_file\n",
    "\n",
    "        # A tokenizer is a function that takes as input a text (possibly multiple sentences) and returns all \n",
    "        # containing tokens (a token is the unit we are going to train the LDA on, can be either a single \n",
    "        # word, a words stem or a word phrase) as an array of strings.\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # The `TextCorpus` class is going to create a dictionary on all tokens of all documents we got. The \n",
    "        # tokens for every document are provided in the `get_texts` function. \n",
    "        super(AmazonCorpus, self).__init__(input=True)\n",
    "\n",
    "        # Ignore common stop words (words that don't carry much meaning) lime 'the' or 'is'\n",
    "        self.dictionary.filter_extremes(no_below=3, no_above=0.2)\n",
    "\n",
    "    # Provides an array of arrays of all the tokens for all documents.\n",
    "    # Example:\n",
    "    #   Let documents be \n",
    "    #     `[\"Hello world. I am doc1.\", \"Nice code! I like it.\"]` \n",
    "    #   In that case the function will yield two arrays with each cell containing the tokens of the sentence\n",
    "    #     `[[\"hello\", \"world\", \".\", \"I\", \"am\", \"doc1\", \".\"], [\"Nice\", \"code\", \"!\", \"I\", \"like\", \"it\", \".\"]]`\n",
    "    def get_texts(self):\n",
    "        with codecs.open(self.question_file, 'r', 'utf-8') as questions:\n",
    "            for question in questions:\n",
    "                yield list(self.tokenizer(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = utils.simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = AmazonCorpus('data/headphone_sents.txt', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model = models.LdaMulticore(corpus=corpus, iterations=3000, chunksize=5000, num_topics=100, id2word=corpus.dictionary, eval_every=3, workers=5)\n",
    "model = models.LdaMulticore(corpus=corpus, iterations=5000, chunksize=100, passes=3, num_topics=10, id2word=corpus.dictionary, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.021*months + 0.018*working + 0.015*product + 0.013*now + 0.012*first + 0.012*broke + 0.011*bought + 0.011*back + 0.010*warranty + 0.009*new',\n",
       " '0.016*head + 0.010*re + 0.009*he + 0.009*wear + 0.009*little + 0.009*around + 0.007*off + 0.007*nice + 0.007*cord + 0.007*over',\n",
       " '0.020*beats + 0.011*love + 0.010*by + 0.010*she + 0.010*best + 0.009*am + 0.009*amazing + 0.008*got + 0.008*look + 0.007*buy',\n",
       " '0.018*phone + 0.017*headset + 0.014*bluetooth + 0.013*mic + 0.013*volume + 0.012*button + 0.009*cable + 0.008*microphone + 0.008*iphone + 0.008*control',\n",
       " '0.008*by + 0.007*set + 0.007*off + 0.007*been + 0.006*first + 0.006*into + 0.006*through + 0.006*work + 0.005*then + 0.005*using',\n",
       " '0.027*earphones + 0.021*tips + 0.011*remote + 0.011*their + 0.009*colors + 0.009*foam + 0.009*company + 0.008*we + 0.008*klipsch + 0.008*set',\n",
       " '0.029*volume + 0.027*cord + 0.016*control + 0.010*clip + 0.009*wire + 0.009*into + 0.009*down + 0.008*cable + 0.007*pull + 0.007*rain',\n",
       " '0.050*noise + 0.019*hear + 0.012*bose + 0.009*running + 0.007*listening + 0.007*outside + 0.006*without + 0.006*canceling + 0.006*block + 0.006*background',\n",
       " '0.007*most + 0.007*headphone + 0.007*high + 0.006*range + 0.006*listening + 0.006*bit + 0.006*sony + 0.006*highs + 0.006*treble + 0.005*their',\n",
       " '0.030*earbuds + 0.024*buds + 0.012*cord + 0.010*earphones + 0.008*ones + 0.008*stay + 0.007*re + 0.007*love + 0.006*come + 0.006*earbud']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
